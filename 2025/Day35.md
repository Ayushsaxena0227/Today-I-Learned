.â€¯slice() â€”â€¯Nonâ€‘destructive â€œcopyâ€ method
ğŸ§  Meaning:
â€œGive me a portion of something, but donâ€™t change the original.â€

Works on
âœ… Strings
âœ… Arrays

ğŸ”¹ Forâ€¯a string
JavaScript

const word = "Programming";
const part = word.slice(0, 3); // "Pro"
Returns a new string "Pro".
Original "Programming" stays intact.
ğŸ”¹ Forâ€¯an array
JavaScript

const nums = [10, 20, 30, 40, 50];
const sliced = nums.slice(1, 4); // [20, 30, 40]

console.log(nums); // [10, 20, 30, 40, 50] (unchanged!)
It extracts items fromâ€¯startâ€¯toâ€¯(endâ€¯âˆ’â€¯1)â€¯â€” identical to your string case.

ğŸ” Key:â€¯Returns a new array / string, never mutates.

ğŸ§© 2.â€¯splice()â€¯â€”â€¯Destructive â€œsurgeryâ€ method (for arrays only)
ğŸ§  Meaning:
â€œCut into this array, optionally remove, and insert new elements.â€

âŒ Works only on
arraysâ€¯â€”â€¯notâ€¯strings.

Example:

JavaScript

const arr = [10, 20, 30, 40, 50];

// remove 2 items starting from index 1
const removed = arr.splice(1, 2);

console.log(removed); // [20, 30]
console.log(arr); // [10, 40, 50] (MUTATED!)
It modifiesâ€¯arrâ€¯directly.

You can also insert new elements:

JavaScript

arr.splice(1, 0, 25, 35);
// starting at index 1, remove 0 items, insert 25 and 35
// arr â†’ [10, 25, 35, 40, 50]
fetch(`/api/users?page=${page}&limit=20`)
isnâ€™t magic by itself; itâ€™s a hint to your backend API that says:

â€œHey server, give me just pageâ€¯N of users, 20â€¯records per page.â€

Which means yes â€” your backend must be built to understandâ€¯pageâ€¯andâ€¯limitâ€¯query parameters, then slice (or paginate) the database results accordingly before sending them to the frontend.

Letâ€™s see what that looks like with a concrete example.

ğŸ§© 1ï¸âƒ£â€¯Frontend behaviour
In your React code:

React

const [page, setPage] = useState(1);

useEffect(() => {
fetch(`/api/users?page=${page}&limit=20`)
.then(res => res.json())
.then(data => setUsers(data.users));
}, [page]);
When the user clicks â€œNext,â€ you increase page byâ€¯1.
The frontend calls the same endpoint again with ?page=2&limit=20.
You just render what comes back.
So the heavy lifting happens serverâ€‘side.

âš™ï¸ 2ï¸âƒ£â€¯Backend design pattern
Assume Nodeâ€¯+â€¯Expressâ€¯+â€¯a MongoDB/Mongoose backend (pattern is identical in SQL, Python, etc.).

Sample endpoint
JavaScript

// GET /api/users?page=2&limit=20
app.get("/api/users", async (req, res) => {
const page = parseInt(req.query.page) || 1;
const limit = parseInt(req.query.limit) || 20;
const skip = (page - 1) \* limit;

const users = await User.find() // or SQL: SELECT \* FROM users
.skip(skip) // skip previous records
.limit(limit); // get this many

const total = await User.countDocuments(); // for total_pages calc

res.json({
users,
total,
totalPages: Math.ceil(total / limit),
currentPage: page,
});
});
Explanation
Parameter Meaning
page which block of results to fetch
limit how many per block
skip used by database to ignore records before that page
So:

pageâ€¯=â€¯1â€¯â†’â€¯skipâ€¯0,â€¯limitâ€¯20
pageâ€¯=â€¯2â€¯â†’â€¯skipâ€¯20,â€¯limitâ€¯20
pageâ€¯=â€¯3â€¯â†’â€¯skipâ€¯40,â€¯limitâ€¯20 â€¦ and so on.
The database returns only that subset, and the frontend renders it instantly because itâ€™s tiny.

ğŸ§  3ï¸âƒ£â€¯Why this design rocks
Benefit Explanation
ğŸš€â€¯Fast Each response small â†’ faster transfer + faster render
ğŸ“‰â€¯Lightweight Browser doesnâ€™t hold the whole millionâ€‘row dataset
ğŸ”â€¯Reusable Same API supports multiple clients (web, mobile, etc.)
ğŸ§©â€¯Scalable Works no matter how big your database grows
Bonus tip
You can also add sort, search, filter as additional query parameters:

Bash

/api/users?page=3&limit=20&sort=name&order=asc&filter=city:Delhi
Your backend parses them and adjusts the database query accordingly â€” classic RESTful design.

ğŸª„ TL;DR
Yes â€” if your frontend calls:

JavaScript

fetch(`/api/users?page=${page}&limit=20`)
then your backend must implement pagination logic using those parameters.

Thatâ€™s the dividedâ€‘labor pattern of mature web apps:

Frontendâ€¯â†’â€¯controls which page to request
Backendâ€¯â†’â€¯returns just that portion of the dataset
